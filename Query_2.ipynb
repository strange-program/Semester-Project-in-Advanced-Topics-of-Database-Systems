{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9fc6d-9f5d-4acc-8353-16449e321cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abb549d2-2905-47de-9cce-d8e7a3ebec60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9c57bf90bb4331bf392e89bef6b0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-----+-------+\n",
      "|year|Vict Descent|Total|Percent|\n",
      "+----+------------+-----+-------+\n",
      "|2010|H           |73558|38.93  |\n",
      "|2010|W           |53835|28.49  |\n",
      "|2010|B           |33937|17.96  |\n",
      "|2011|H           |70845|38.8   |\n",
      "|2011|W           |51219|28.05  |\n",
      "|2011|B           |32579|17.84  |\n",
      "|2012|H           |70338|38.25  |\n",
      "|2012|W           |51839|28.19  |\n",
      "|2012|B           |33572|18.26  |\n",
      "|2013|H           |66741|37.97  |\n",
      "|2013|W           |48453|27.57  |\n",
      "|2013|B           |31975|18.19  |\n",
      "|2014|H           |68763|38.42  |\n",
      "|2014|W           |47531|26.56  |\n",
      "|2014|B           |32952|18.41  |\n",
      "|2015|H           |55978|36.65  |\n",
      "|2015|W           |44102|28.87  |\n",
      "|2015|B           |26510|17.35  |\n",
      "|2016|H           |99135|38.74  |\n",
      "|2016|W           |63760|24.92  |\n",
      "|2016|B           |42449|16.59  |\n",
      "|2017|H           |78308|37.55  |\n",
      "|2017|W           |52744|25.29  |\n",
      "|2017|B           |34713|16.65  |\n",
      "|2018|H           |75958|36.42  |\n",
      "|2018|W           |52233|25.05  |\n",
      "|2018|B           |35340|16.95  |\n",
      "|2019|H           |72458|36.38  |\n",
      "|2019|W           |48863|24.54  |\n",
      "|2019|B           |33157|16.65  |\n",
      "|2020|H           |61606|35.33  |\n",
      "|2020|W           |42638|24.45  |\n",
      "|2020|B           |28785|16.51  |\n",
      "|2021|H           |63676|35.08  |\n",
      "|2021|W           |44523|24.53  |\n",
      "|2021|B           |30173|16.62  |\n",
      "|2022|H           |73111|35.64  |\n",
      "|2022|W           |46695|22.76  |\n",
      "|2022|B           |34634|16.88  |\n",
      "|2023|H           |69401|34.55  |\n",
      "|2023|W           |44615|22.21  |\n",
      "|2023|B           |30504|15.19  |\n",
      "|2024|H           |28576|29.05  |\n",
      "|2024|W           |22958|23.34  |\n",
      "|2024|X           |19984|20.32  |\n",
      "|2025|H           |34   |40.48  |\n",
      "|2025|X           |24   |28.57  |\n",
      "|2025|W           |13   |15.48  |\n",
      "+----+------------+-----+-------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, DoubleType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Dataframe query 2 execution with SQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the data schema\n",
    "data_schema = StructType([\n",
    "    StructField(\"DR_NO\", IntegerType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", IntegerType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", IntegerType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", IntegerType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType()),\n",
    "])\n",
    "\n",
    "# Load data from the 2 buckets and combine them\n",
    "data1 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\", \\\n",
    "    header=False, \\\n",
    "    schema=data_schema)\n",
    "\n",
    "data2 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", \\\n",
    "    header=False, \\\n",
    "    schema=data_schema)\n",
    "\n",
    "df = data1.union(data2)\n",
    "df.createOrReplaceTempView(\"crimes\")\n",
    "\n",
    "# Extract year of occurence\n",
    "query0 = \"SELECT *, year(to_timestamp(`DATE OCC`, 'yyyy MMM dd hh:mm:ss a')) AS year \\\n",
    "FROM crimes;\"\n",
    "df_with_year = spark.sql(query0)\n",
    "df_with_year.createOrReplaceTempView(\"crimes\")\n",
    "\n",
    "# Query 1: group data by year and ethnicity\n",
    "# Exclude null data\n",
    "query1 = \"SELECT year, `Vict Descent`, COUNT(*) AS Total FROM crimes \\\n",
    "            WHERE year IS NOT NULL AND `Vict Descent` IS NOT NULL \\\n",
    "            GROUP BY year, `Vict Descent`\"\n",
    "\n",
    "ordered_by_year_and_ethn = spark.sql(query1)\n",
    "\n",
    "# Save the query\n",
    "ordered_by_year_and_ethn.createOrReplaceTempView(\"modified_crimes\")\n",
    "\n",
    "# Query 2: Partition previous result over years, then sort, take the top 3 and calculate percentage\n",
    "query2 = \"SELECT year, `Vict Descent`, Total, ROUND(100*(Total/sum),2) as Percent FROM ( \\\n",
    "    SELECT *, ROW_NUMBER() OVER ( \\\n",
    "               PARTITION BY year \\\n",
    "               ORDER BY Total DESC \\\n",
    "           ) AS rn, SUM(Total) OVER (PARTITION BY year) as sum\\\n",
    "    FROM modified_crimes \\\n",
    ") AS t \\\n",
    "WHERE rn <= 3 ORDER BY year, Total DESC;\"\n",
    "\n",
    "res = spark.sql(query2)\n",
    "res.show(1000, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029ce3d-d2da-4ac4-bed9-ddda253dce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2dbc290-cca0-49b8-9bd8-efa6fb608428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db9e5e0232841459b0a5fcf4da20e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-----+-------+\n",
      "|year|Vict Descent|count|percent|\n",
      "+----+------------+-----+-------+\n",
      "|2010|H           |73558|38.93  |\n",
      "|2010|W           |53835|28.49  |\n",
      "|2010|B           |33937|17.96  |\n",
      "|2011|H           |70845|38.8   |\n",
      "|2011|W           |51219|28.05  |\n",
      "|2011|B           |32579|17.84  |\n",
      "|2012|H           |70338|38.25  |\n",
      "|2012|W           |51839|28.19  |\n",
      "|2012|B           |33572|18.26  |\n",
      "|2013|H           |66741|37.97  |\n",
      "|2013|W           |48453|27.57  |\n",
      "|2013|B           |31975|18.19  |\n",
      "|2014|H           |68763|38.42  |\n",
      "|2014|W           |47531|26.56  |\n",
      "|2014|B           |32952|18.41  |\n",
      "|2015|H           |55978|36.65  |\n",
      "|2015|W           |44102|28.87  |\n",
      "|2015|B           |26510|17.35  |\n",
      "|2016|H           |99135|38.74  |\n",
      "|2016|W           |63760|24.92  |\n",
      "|2016|B           |42449|16.59  |\n",
      "|2017|H           |78308|37.55  |\n",
      "|2017|W           |52744|25.29  |\n",
      "|2017|B           |34713|16.65  |\n",
      "|2018|H           |75958|36.42  |\n",
      "|2018|W           |52233|25.05  |\n",
      "|2018|B           |35340|16.95  |\n",
      "|2019|H           |72458|36.38  |\n",
      "|2019|W           |48863|24.54  |\n",
      "|2019|B           |33157|16.65  |\n",
      "|2020|H           |61606|35.33  |\n",
      "|2020|W           |42638|24.45  |\n",
      "|2020|B           |28785|16.51  |\n",
      "|2021|H           |63676|35.08  |\n",
      "|2021|W           |44523|24.53  |\n",
      "|2021|B           |30173|16.62  |\n",
      "|2022|H           |73111|35.64  |\n",
      "|2022|W           |46695|22.76  |\n",
      "|2022|B           |34634|16.88  |\n",
      "|2023|H           |69401|34.55  |\n",
      "|2023|W           |44615|22.21  |\n",
      "|2023|B           |30504|15.19  |\n",
      "|2024|H           |28576|29.05  |\n",
      "|2024|W           |22958|23.34  |\n",
      "|2024|X           |19984|20.32  |\n",
      "|2025|H           |34   |40.48  |\n",
      "|2025|X           |24   |28.57  |\n",
      "|2025|W           |13   |15.48  |\n",
      "+----+------------+-----+-------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, DoubleType, StringType\n",
    "from pyspark.sql.functions import col, row_number, desc, sum, round\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Dataframe query 2 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the data schema\n",
    "data_schema = StructType([\n",
    "    StructField(\"DR_NO\", IntegerType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", IntegerType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", IntegerType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", IntegerType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType()),\n",
    "])\n",
    "\n",
    "# Load data from the 2 buckets and combine them\n",
    "data1 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\", \\\n",
    "    header=False, \\\n",
    "    schema=data_schema)\n",
    "\n",
    "data2 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", \\\n",
    "    header=False, \\\n",
    "    schema=data_schema)\n",
    "\n",
    "data = data1.union(data2)\n",
    "\n",
    "# Extract year of occurence\n",
    "df_with_year = data.withColumn(\n",
    "    \"year\",\n",
    "    F.year(F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\"))\n",
    ")\n",
    "df_with_year.createOrReplaceTempView(\"crimes\")\n",
    "\n",
    "# Group by year and ethnicity, then remove null values\n",
    "year_descent_grouped = df_with_year.groupBy(col(\"year\"),col(\"Vict Descent\")).count()\n",
    "year_descent_grouped = year_descent_grouped.na.drop()\n",
    "\n",
    "# Define window for ordered descent counts per year\n",
    "windowSpec = Window.partitionBy(\"year\").orderBy(desc(\"count\"))\n",
    "\n",
    "# Define a window for total per year (no ordering needed)\n",
    "total_window = Window.partitionBy(\"year\")\n",
    "\n",
    "# Add row_number and total per year\n",
    "# Then, get the top 3 rows and calculate the percent\n",
    "df_final = year_descent_grouped.withColumn(\"rank\", row_number().over(windowSpec)) \\\n",
    "                          .withColumn(\"total_count\", sum(\"count\").over(total_window)) \\\n",
    "                          .filter(col(\"rank\") <= 3) \\\n",
    "                          .withColumn(\"percent\", round(100 * col(\"count\") / col(\"total_count\"),2)) \\\n",
    "                          .drop(\"rank\") \\\n",
    "                          .drop(\"total_count\")\n",
    "\n",
    "df_final.show(1000, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad092b-4bf6-4777-a48e-92fbd2686d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
