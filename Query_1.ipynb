{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeeff659",
   "metadata": {},
   "source": [
    "# QUERY 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0683053",
   "metadata": {},
   "source": [
    "## RRD Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0faf4aba-2960-461c-9a5f-e988241b95f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1805</td><td>application_1765289937462_1789</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1789/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-141.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1789_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1807</td><td>application_1765289937462_1791</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1791/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1791_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1808</td><td>application_1765289937462_1792</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1792/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1792_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f58ad3-d291-490a-9986-d9799f947ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1809</td><td>application_1765289937462_1793</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1793/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-251.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1793_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd1372f6b6f40b6987ca8055a842f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927d54d69e9141108cc393a6761ac654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('adult', 121660), ('young adult', 33758), ('child', 16014), ('senior', 6011)]\n",
      "Execution Time: 37.659892082214355 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, DoubleType, StringType\n",
    "import time\n",
    "\n",
    "# After initilizing the Spark Session, we can start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Reading the 2 CSV files needed for the query\n",
    "df1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=False,\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "df2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=False,\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "# Converting DataFrames to RDDs and merging them to have the LA Crime Data from 2010 to 2025\n",
    "data1 = df1.rdd\n",
    "data2 = df2.rdd\n",
    "data = data1.union(data2)\n",
    "\n",
    "# Filtering the data to keep only the aggravated assault crimes\n",
    "aggravated_assaults = data.filter(lambda x: \"AGGRAVATED ASSAULT\" in x[9])\n",
    "\n",
    "\n",
    "# The User Defined Function to categorize ages into groups and count occurrences\n",
    "def get_age_group(x):\n",
    "    age = int(x[11])\n",
    "    if age < 18:\n",
    "        return [\"child\", 1]\n",
    "    elif age >= 18 and age <= 24:\n",
    "        return [\"young adult\", 1]\n",
    "    elif age >= 25 and age <= 64:\n",
    "        return [\"adult\", 1]\n",
    "    else:\n",
    "        return [\"senior\", 1]\n",
    "\n",
    "\n",
    "# Applying the UDF and grouping the data by age groups\n",
    "assault_age_group = aggravated_assaults.map(get_age_group)\n",
    "age_group_count = assault_age_group.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# Sorting the results in descending order based on the count of aggravated assaults\n",
    "sorted_data = age_group_count.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Collecting and printing the final results\n",
    "print(sorted_data.collect())\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1f29a",
   "metadata": {},
   "source": [
    "## Dataframe implementation (with UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a1fb58-fca2-4a42-adfa-110bc4b263b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1812</td><td>application_1765289937462_1796</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1796/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1796_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5317dd7ef25f4133bd4ef1a6fb8930f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1805</td><td>application_1765289937462_1789</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1789/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-141.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1789_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1807</td><td>application_1765289937462_1791</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1791/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1791_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1808</td><td>application_1765289937462_1792</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1792/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1792_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1811</td><td>application_1765289937462_1795</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1795/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1795_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1812</td><td>application_1765289937462_1796</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1796/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1796_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5948b147-34e3-4343-9dbc-d71a699dbc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932ddd4ef987483d8ceadabf6ede0668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, DoubleType, StringType\n",
    "\n",
    "# Defining the schema for the CSV files\n",
    "data_schema = StructType([\n",
    "    StructField(\"DR_NO\", IntegerType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", IntegerType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", IntegerType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", IntegerType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "938e0e40-28c2-4aef-9b6a-121d6504df48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2efa28b9f045cabde355ab4fd349e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|  age group| count|\n",
      "+-----------+------+\n",
      "|      adult|121660|\n",
      "|young adult| 33758|\n",
      "|      child| 16014|\n",
      "|     senior|  6011|\n",
      "+-----------+------+\n",
      "\n",
      "Execution Time: 15.84756088256836 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, DoubleType, StringType\n",
    "from pyspark.sql.functions import col, udf\n",
    "import time\n",
    "\n",
    "# After initilizing the Spark Session, we can start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Reading the CSV files into DataFrames with the defined schema\n",
    "data1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\", \n",
    "    header=False, \n",
    "    schema=data_schema)\n",
    "\n",
    "data2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", \n",
    "    header=False, \n",
    "    schema=data_schema)\n",
    "\n",
    "data = data1.union(data2)\n",
    "\n",
    "# Filtering the data to keep only the aggravated assault crimes\n",
    "aggravated_assaults = data.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "\n",
    "# The User Defined Function to categorize ages into groups\n",
    "def get_group(x):\n",
    "    if x < 18:\n",
    "        return \"child\"\n",
    "    elif x >= 18 and x <= 24:\n",
    "        return \"young adult\"\n",
    "    elif x >= 25 and x <= 64:\n",
    "        return \"adult\"\n",
    "    return \"senior\"\n",
    "\n",
    "\n",
    "get_group_udf = udf(get_group, StringType())\n",
    "\n",
    "# Applying the UDF and grouping the data by age groups\n",
    "results = aggravated_assaults.withColumn(\"age group\", get_group_udf(col(\"Vict Age\"))).groupBy(col(\"age group\")).count()\n",
    "results_sorted = results.sort(col(\"count\"), ascending=False).show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97935260",
   "metadata": {},
   "source": [
    "## Dataframe implementation (without UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17da46d-0c70-43b4-b28d-e7192d2c1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1813</td><td>application_1765289937462_1797</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1797/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1797_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ae5b526fab49648f556ba2addc6a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1805</td><td>application_1765289937462_1789</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1789/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-141.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1789_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1807</td><td>application_1765289937462_1791</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1791/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1791_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1808</td><td>application_1765289937462_1792</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1792/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1792_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1811</td><td>application_1765289937462_1795</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1795/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1795_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1813</td><td>application_1765289937462_1797</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1797/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1797_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e052ed-b933-4823-a38f-e03712ee2b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169a16367a86458aa075a7a08580f614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, DoubleType, StringType\n",
    "\n",
    "# Defining the schema for the CSV files\n",
    "data_schema = StructType([\n",
    "    StructField(\"DR_NO\", IntegerType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", IntegerType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", IntegerType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", IntegerType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc769b2a-701a-4900-a1cc-66c3f71c5c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea20cc97653f4d988f4077ba462c3343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|  age group| count|\n",
      "+-----------+------+\n",
      "|      adult|121660|\n",
      "|young adult| 33758|\n",
      "|      child| 16014|\n",
      "|     senior|  6011|\n",
      "+-----------+------+\n",
      "\n",
      "Execution Time: 13.86941146850586 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, count, when, expr\n",
    "import time\n",
    "\n",
    "# After initilizing the Spark Session, we can start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Reading the CSV files into DataFrames with the defined schema\n",
    "data1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    header=False,\n",
    "    schema=data_schema)\n",
    "\n",
    "data2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\n",
    "    header=False,\n",
    "    schema=data_schema)\n",
    "\n",
    "# Merging the two DataFrames to have the LA Crime Data from 2010 to 2025\n",
    "data = data1.union(data2)\n",
    "\n",
    "# Filtering the data to keep only the aggravated assault crimes\n",
    "aggravated_assaults = data.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "updated_df = aggravated_assaults.withColumn(\n",
    "    \"age group\",\n",
    "    when(col(\"Vict Age\") < 18, \"child\")\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") < 25), \"young adult\")\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") < 65), \"adult\")\n",
    "    .otherwise(\"senior\")\n",
    ")\n",
    "\n",
    "age_group_count = updated_df.groupBy(\"age group\").count()\n",
    "age_group_count = age_group_count.orderBy(col(\"count\").desc())\n",
    "age_group_count.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
